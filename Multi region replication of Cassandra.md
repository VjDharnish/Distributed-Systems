
Apache Cassandra relies on Amazon's Dynamo DB key-value store architecture
Dynamo system has three main components:
	- Request coordination over a partitioned dataset
	- Ring membership and failure detection
	- A local persistence (storage) engine

**Multi-Master Architecture:** 
Unlike many relational databases (Active-Passive), DynamoDB Global Tables are **Active-Active**. You can write to the same item in Region A and Region B simultaneously.
But Cassandra does have complete leaderless architecture for both within and multi-region cluster replication

## Inter DC Connection

Identify the nodes of another DC cluster by gossip only and then operate independently (Not form a global cluster). The gossip is achieved by same cluster name
Once Node identified by the gossip, it exchange message heartbeat state and application state. This is where DC connects No cluster barrier no state exchange.
It finally forms a ring of nodes. The ring is global

# How Data Write happens

The Coordinator from Primary DC receives request
Fetch ReplicaPlan - computes what are the replicas participate(DC- aware)
Fetch NetworkTopologyStrategy - container required DCs and their respective Replicas
Fetch ConsistencyLevel  
Send to local DC, wait for ACK. send to Remote DC and forgets if LOCAL_QUORUM
Append to commitlog and apply to memtable . Ack is sent after commitlog write
Coordinator send response once ack received
### What if DC2 is Down
 The datawrite call stored in localHint Untill DC2 returns
### Conflict Resolution
The datalog with highest timestamp wins

### Read Repair Happens automatically across DCs as same as how it happens within cluster



# How Schema Changes happen
Goal :
The schema change must appear everywhere or nowhere. There is no eventual consistency for schema. Because schema is the only strongly consistent global state.
if schema is inconsistent:
- Writes/reads are blocked and cluster is considered is unhealthy


Once Schema change received on Coordinator
PART - 1
- Schema mutation(insert/delete/update..) written to local system_schema.* tables
- stored in commitlog
- applied to memtables
- At this point , schema is changed only on this node

PART - 2
- Each schema has a version which is generated by schema metadata,  The version is gossiped to other nodes ( and DC2 nodes)

PART - 3
- Mismatch detected  
- Once mismatch found in a gossip , it triggers schema pull (which is handled automatically as basic gossip failover)

PART - 4
- DC2 pull updated schema from DC1
- Write to system_schema.* tables
- Update to in-memory schema

PART - 5
- Verify schema agreement across cluster
	- Query all live nodes
	- compare schema version
	- retry untill all agree/ timeout
- Response given once schema change completed in all nodes.
- If DC2 is slow.
	- Client waits and writes are blocked in DC1
- Persist Node identity and state to
	- Local
	- peers
	- peers_v2 - Newer metadata
	- topology - DC/rack
###  DC2 Failure and recovery state
- Schema change stored as local hint state. Not global
- DC2 marked as Schema Disagreement
- Write fails in DC1 in UNavailable Exception
- When DC2 comes back: Gossip sees mismatch and schema pull happens automatically.

## Why Schema never diverge
| Data                    | Schema              |
| ----------------------- | ------------------- |
| Eventually consistent   | Strongly consistent |
| Timestamp-based         | Version-hash-based  |
| Repair heals            | Pull-only           |
| Can diverge temporarily | Cannot              |

# Key Mental Model

- Cassandra DCs dont join eachother. They learn about eachother
-  Cassandra treats **schema as control-plane state**, not data.
- It doesnot replicate data , it replicates drift (by WAL) and then pulls them together
### Overall
Cassandra does not synchronize DCs.
It lets
- Accepts writes independently
- drift temporarily
- and continuous recouncil
that why it survives
- WAN Latency
- DC Outages
- Clock Skew










